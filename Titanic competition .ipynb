{"cells":[{"metadata":{},"cell_type":"markdown","source":"**The Exercise**\n\nThis exercise from kaggle.com (a website for ML and DS challenges) provides us with train.csv and test.csv that contain a lot of information about the passenger of the famous Titanic. The train.csv includes the information if a passenger has survived or not. The goal is to predict which of the passengers, listed in test.csv survived the titanic disaster.\n\nFurther information about the data set and its features: https://www.kaggle.com/c/titanic/data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\n\nfrom sklearn import linear_model\nfrom sklearn.preprocessing import scale\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Get data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#loading data\ntrain_df = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/titanic/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Get to know the data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#take a look at the feature correlation (of the numeric features)\ncorr = train_df.corr()\ncorr.style.background_gradient(cmap='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data pre-processing"},{"metadata":{},"cell_type":"markdown","source":"The name feature is obviously irrelevant for survival, but contains a Persons Title, which could be an indicater for a higher/lower priority. So we create a 'Title' feature for train_df and test_df"},{"metadata":{"trusted":true},"cell_type":"code","source":"#insert a 'Title' feature from the name feature because it probably correlates with survival\ntrain_df['Title'] = train_df['Name'].str.split(\", \", expand=True)[1].str.split(\". \", expand=True)[0].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#take a look at the result\ntrain_df['Title']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#looks like there are many different Titles\n#see which title appears how often\ntrain_df['Title'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#because there are a lot of different features, we can summarize the rare ones in 'else'\n#We set the threshold to 10, so we set every Title that appears less than 10 times to 'else'\ntrain_df.loc[(train_df['Title'] != 'Mr') & (train_df['Title'] != 'Miss') & (train_df['Title'] != 'Mrs') & (train_df['Title'] != 'Master'), 'Title'] = 'else'\ntrain_df['Title'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#do the same for test_df\n#get titles\ntest_df['Title'] = test_df['Name'].str.split(\", \", expand=True)[1].str.split(\". \", expand=True)[0]\n\n#see which value appears how often\ntest_df['Title'].value_counts()\n\n#set every title that appears less than 10 times to 'else'\ntest_df.loc[(test_df['Title'] != 'Mr') & (test_df['Title'] != 'Miss') & (test_df['Title'] != 'Mrs') & (test_df['Title'] != 'Master'), 'Title'] = 'else'\ntest_df['Title'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Title feature created"},{"metadata":{"trusted":true},"cell_type":"code","source":"#the features SibSp and Parch can be summarized as FamilySize (split testing showed, that this feature indeed improves the models performance)\n#add a FamilySize feature:\ntrain_df['FamilySize'] = train_df['SibSp'] + train_df['Parch']\ntest_df['FamilySize'] = test_df['SibSp'] + test_df['Parch']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#that fact that a person is traveling with/without family members could be correlated to survival (split testing showed, that this feature indeed improves the models performance)\n#add an isAlone feature which is 1, when FamilySize is 0\ntrain_df.loc[train_df['FamilySize'] == 0, 'isAlone'] = 1\ntest_df.loc[test_df['FamilySize'] == 0, 'isAlone'] = 1\ntrain_df.loc[train_df['isAlone'] != 1, 'isAlone'] = 0\ntest_df.loc[test_df['isAlone'] != 1, 'isAlone'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check result\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#take another look at the feature correlation, including the new features\ncorr = train_df.corr()\ncorr.style.background_gradient(cmap='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop columns that have mostly missing entries (like Cabin) and/or are irrelevant for survival\n#PassengerId in test_df is still needed for the submission in the end\ntrain_df = train_df.drop(columns=['PassengerId', 'Name', 'Ticket', 'Cabin'])\ntest_df = test_df.drop(columns=['Name', 'Ticket', 'Cabin'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#convert sex-feature in categorical int. female=1, male=0\ntrain_df = train_df.replace({'female':1,'male':0})\ntest_df = test_df.replace( {'female':1,'male':0})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#rename Sex column in Gender\ntrain_df = train_df.rename(columns={'Sex' : 'Gender'})\ntest_df = test_df.rename(columns={'Sex' : 'Gender'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#convert Title feature to cetegorial int:\ntrain_df['Title'] = train_df['Title'].replace({'Mr':'1', 'Miss':2, 'Mrs':3, 'Master':4, 'else':5 }).astype(int)\ntest_df['Title'] = test_df['Title'].replace({'Mr':'1', 'Miss':2, 'Mrs':3, 'Master':4, 'else':5 }).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check correlation map again with all features being numerical now\ncorr = train_df.corr()\ncorr.style.background_gradient(cmap='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"looks like Survived and Title correlates pretty heavily, while the other new features are just slightly correlating"},{"metadata":{},"cell_type":"markdown","source":"# **Check training data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#check NaN values in training data\ntrain_df[train_df.isna().any(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Seems like mainly ages are missing. So I have to make assumptions"},{"metadata":{},"cell_type":"markdown","source":"If you take a look at the correlation map, you can see that Age mainly depends on Pclass, SibSp and Parch (And on FamilySize and IsAlone, but these are depending on SibSp and Parch, so I won't consider them further)\nSo Passengers will be grouped by these features and missing ages are set to the mean value of their group"},{"metadata":{"trusted":true},"cell_type":"code","source":"#set NaN ages to the mean of the group they belong to\nfor i_class in range(0,4):\n    for i_Sib in range(0,9):\n        for i_Parch in range(0,3):\n            \n            mean_group_age = train_df.loc[(train_df['Pclass'] == i_class) & (train_df['SibSp'] == i_Sib) & (train_df['Parch'] == i_Parch) & (train_df['Age'].isna()==False)]['Age'].mean()\n            \n            if math.isnan(mean_group_age)==False:\n                mean_group_age=int(mean_group_age)\n                \n                train_df.loc[(train_df['Pclass'] == i_class) & (train_df['SibSp'] == i_Sib) & (train_df['Parch'] == i_Parch) & (train_df['Age'].isna()) ,'Age'] = mean_group_age","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check for NaN values again\ntrain_df.loc[train_df['Age'].isna()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"seems like we got only members of one family left / They did not get an age because their was no row with that combination of Pclass, SibSp and Parch AND a valid age, so no mean age could be calculated for that group. I will just set their age to the mean age of their passengers class"},{"metadata":{"trusted":true},"cell_type":"code","source":"#set remaining NaN ages to their Pclasses mean\ntrain_df.loc[train_df['Age'].isna(), ['Age']] = train_df.loc[train_df['Pclass'] == 3]['Age'].mean()\ntrain_df.loc[train_df['Age'].isna()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#no missing ages left\n#check for any NaN entries:\ntrain_df[train_df.isna().any(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2 rows without \"Embarked\" / I will set them manually to the most likely value, which is the one that occured most often\ntrain_df.groupby('Embarked')['Age'].count() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#S is by far the mostly appearing entry, so I will set the NaN's to S too\ntrain_df.loc[train_df['Embarked'].isna() == True, 'Embarked'] = str('S')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check for missing values again\ntrain_df[train_df.isna().any(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Noe more NaN values! :)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#take another look at the training data\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#make Embarked a categorical int feature\ntrain_df['Embarked']=train_df['Embarked'].map({'S':1,'C':2,'Q':3})\ntrain_df.head(50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"looks like our training data is ready to go"},{"metadata":{},"cell_type":"markdown","source":"# **Check Test data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#check for NaNs\ntest_df[test_df.isna().any(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#seems like many ages are missing again, so we use the same code like before, just on the training data\n#set NaN ages to the mean of the group they belong to\nfor i_class in range(0,4):\n    for i_Sib in range(0,9):\n        for i_Parch in range(0,3):\n            \n            mean_group_age = test_df.loc[(test_df['Pclass'] == i_class) & (test_df['SibSp'] == i_Sib) & (test_df['Parch'] == i_Parch) & (test_df['Age'].isna()==False)]['Age'].mean()\n            \n            \n            if math.isnan(mean_group_age)==False:\n                mean_group_age=int(mean_group_age)\n                \n                test_df.loc[(test_df['Pclass'] == i_class) & (test_df['SibSp'] == i_Sib) & (test_df['Parch'] == i_Parch) & (test_df['Age'].isna()) ,'Age'] = mean_group_age","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check for nans in Age again\ntest_df.loc[test_df['Age'].isna()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"same issue like in train_df. I will set the missing ages to the mean of their Pclass again"},{"metadata":{"trusted":true},"cell_type":"code","source":"#set missing ages to their Pclass means\ntest_df.loc[test_df['Age'].isna() == True, 'Age'] = test_df.loc[test_df['Pclass'] ==3, 'Age'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check for nan again\ntrain_df.loc[train_df['Age'].isna()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#seems like there are no more nan ages\n#now check for nan's in all columns\ntest_df[test_df.isna().any(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#seems like only one Fare value is missing\n#if you take another look at the correlation map, you can see that Fare most heavily depends on Pclass, so I will simply set the missing Fare to the Pclasses mean \ntest_df.loc[test_df['Fare'].isna() == True, 'Fare']=test_df.loc[test_df['Pclass']==3]['Fare'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check for nan in whole df again\ntest_df[test_df.isna().any(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"no more nan! :)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#take another look at the test data\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#convert Embarked feature to categorical int:\ntest_df['Embarked']=test_df['Embarked'].replace({'S':1,'C':2,'Q':3})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#take a final look at training data\ntrain_df.head(50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"looks like the test data is ready to go"},{"metadata":{"trusted":true},"cell_type":"code","source":"#split train_df in train, test and cv data\n\nx_train, x_cv, y_train, y_cv = train_test_split( train_df, train_df['Survived'], test_size=0.2, random_state=1)\n\ny_train=x_train['Survived']\ny_cv=x_cv['Survived']\n\nx_train=x_train.drop(columns=['Survived'])\nx_cv = x_cv.drop(columns=['Survived'])\n\nx_test=test_df.drop(columns=['PassengerId'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#x_train['Title']=x_train['Title'].astype(int)\n#x_train['Embarked']=x_train['Embarked'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#feature scaling\nx_train=scale(x_train)\nx_cv=scale(x_cv)\nx_test=scale(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build models"},{"metadata":{},"cell_type":"markdown","source":"**Logistic regression**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_LG = LogisticRegression()\nmodel_LG.fit(x_train, y_train)\ny_hat_LG = model_LG.predict(x_cv)\nperf_LG = mean_squared_error(y_hat_LG, y_cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"Mean squared error of Logistic regression: \",perf_LG","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Neural Network** "},{"metadata":{},"cell_type":"markdown","source":"I will run cv testing on 9 different models with different architectures (no. of nodes and no. of layers will differ)"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_epochs=500","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#build model_1\nmodel_1=Sequential()\nn_columns = train_df.columns.size -1\n\nmodel_1.add(Dense(5, activation='relu', input_shape=(n_columns,)))\nmodel_1.add(Dense(5, activation='relu'))\nmodel_1.add(Dense(1))\n\nmodel_1.compile(optimizer='adam', loss='mean_squared_error')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train the model\nmodel_1.fit(x_train, y_train, epochs=train_epochs, verbose=0)\ny_hat_1=model_1.predict(x_cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Mean squared error NN-model: \", mean_squared_error(y_cv,y_hat_1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#y_hat_NN = mean_squared_error(y_cv,y_hat_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#build model_2\nmodel_2=Sequential()\nn_columns = train_df.columns.size -1\n\nmodel_2.add(Dense(10, activation='relu', input_shape=(n_columns,)))\nmodel_2.add(Dense(10, activation='relu'))\nmodel_2.add(Dense(1))\n\nmodel_2.compile(optimizer='adam', loss='mean_squared_error')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_2.fit(x_train, y_train, epochs=train_epochs, verbose=0)\ny_hat_2=model_2.predict(x_cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Mean squared error NN-model: \", mean_squared_error(y_cv,y_hat_2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#build model_3\nmodel_3=Sequential()\nn_columns = train_df.columns.size -1\n\nmodel_3.add(Dense(20, activation='relu', input_shape=(n_columns,)))\nmodel_3.add(Dense(20, activation='relu'))\nmodel_3.add(Dense(1))\n\nmodel_3.compile(optimizer='adam', loss='mean_squared_error')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_3.fit(x_train, y_train, epochs=train_epochs, verbose=0)\ny_hat_3=model_3.predict(x_cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Mean squared error: \", mean_squared_error(y_hat_3, y_cv))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#build model_4\nmodel_4=Sequential()\nn_columns = train_df.columns.size -1\n\nmodel_4.add(Dense(5, activation='relu', input_shape=(n_columns,)))\nmodel_4.add(Dense(5, activation='relu'))\nmodel_4.add(Dense(5, activation='relu'))\nmodel_4.add(Dense(5, activation='relu'))\nmodel_4.add(Dense(1))\n\nmodel_4.compile(optimizer='adam', loss='mean_squared_error')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_4.fit(x_train, y_train, epochs=train_epochs, verbose=0)\ny_hat_4=model_4.predict(x_cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"MSE: \",mean_squared_error(y_hat_4, y_cv))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#build model_5\nmodel_5=Sequential()\nn_columns = train_df.columns.size -1\n\nmodel_5.add(Dense(10, activation='relu', input_shape=(n_columns,)))\nmodel_5.add(Dense(10, activation='relu'))\nmodel_5.add(Dense(10, activation='relu'))\nmodel_5.add(Dense(10, activation='relu'))\nmodel_5.add(Dense(1))\n\nmodel_5.compile(optimizer='adam', loss='mean_squared_error')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_5.fit(x_train, y_train, epochs=train_epochs, verbose=0)\ny_hat_5=model_5.predict(x_cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"MSE: \",mean_squared_error(y_hat_5, y_cv))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#build model_6\nmodel_6=Sequential()\nn_columns = train_df.columns.size -1\n\nmodel_6.add(Dense(20, activation='relu', input_shape=(n_columns,)))\nmodel_6.add(Dense(20, activation='relu'))\nmodel_6.add(Dense(20, activation='relu'))\nmodel_6.add(Dense(20, activation='relu'))\nmodel_6.add(Dense(1))\n\nmodel_6.compile(optimizer='adam', loss='mean_squared_error')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_6.fit(x_train, y_train, epochs=train_epochs, verbose=0)\ny_hat_6=model_6.predict(x_cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"MSE: \",mean_squared_error(y_hat_6, y_cv))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#build model_7\nmodel_7=Sequential()\nn_columns = train_df.columns.size -1\n\nmodel_7.add(Dense(5, activation='relu', input_shape=(n_columns,)))\nmodel_7.add(Dense(5, activation='relu'))\nmodel_7.add(Dense(5, activation='relu'))\nmodel_7.add(Dense(5, activation='relu'))\nmodel_7.add(Dense(5, activation='relu'))\nmodel_7.add(Dense(5, activation='relu'))\nmodel_7.add(Dense(1))\n\nmodel_7.compile(optimizer='adam', loss='mean_squared_error')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_7.fit(x_train, y_train, epochs=train_epochs, verbose=0)\ny_hat_7=model_7.predict(x_cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"MSE: \",mean_squared_error(y_hat_7, y_cv))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#build model_8\nmodel_8=Sequential()\nn_columns = train_df.columns.size -1\n\nmodel_8.add(Dense(10, activation='relu', input_shape=(n_columns,)))\nmodel_8.add(Dense(10, activation='relu'))\nmodel_8.add(Dense(10, activation='relu'))\nmodel_8.add(Dense(10, activation='relu'))\nmodel_8.add(Dense(10, activation='relu'))\nmodel_8.add(Dense(10, activation='relu'))\nmodel_8.add(Dense(1))\n\nmodel_8.compile(optimizer='adam', loss='mean_squared_error')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_8.fit(x_train, y_train, epochs=train_epochs, verbose=0)\ny_hat_8=model_8.predict(x_cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"MSE: \",mean_squared_error(y_hat_8, y_cv))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#build model_9\nmodel_9=Sequential()\nn_columns = train_df.columns.size -1\n\nmodel_9.add(Dense(20, activation='relu', input_shape=(n_columns,)))\nmodel_9.add(Dense(20, activation='relu'))\nmodel_9.add(Dense(20, activation='relu'))\nmodel_9.add(Dense(20, activation='relu'))\nmodel_9.add(Dense(20, activation='relu'))\nmodel_9.add(Dense(20, activation='relu'))\nmodel_9.add(Dense(1))\n\nmodel_9.compile(optimizer='adam', loss='mean_squared_error')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_9.fit(x_train, y_train, epochs=train_epochs, verbose=0)\ny_hat_9=model_9.predict(x_cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"MSE: \",mean_squared_error(y_hat_9, y_cv))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"perf_NN=min(mean_squared_error(y_hat_1, y_cv),mean_squared_error(y_hat_2, y_cv),mean_squared_error(y_hat_3, y_cv),mean_squared_error(y_hat_4, y_cv),mean_squared_error(y_hat_5, y_cv),mean_squared_error(y_hat_6, y_cv),mean_squared_error(y_hat_7, y_cv),mean_squared_error(y_hat_8, y_cv),mean_squared_error(y_hat_9, y_cv))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#choose best NN model\nif perf_NN == mean_squared_error(y_hat_1, y_cv):\n    y_hat_NN = np.round(y_hat_1)\n    model_NN = model_1\n    \nif perf_NN == mean_squared_error(y_hat_2, y_cv):\n    y_hat_NN = np.round(y_hat_2)\n    model_NN = model_2\n    \nif perf_NN == mean_squared_error(y_hat_3, y_cv):\n    y_hat_NN = np.round(y_hat_3)\n    model_NN = model_3\n    \nif perf_NN == mean_squared_error(y_hat_4, y_cv):\n    y_hat_NN = np.round(y_hat_4)\n    model_NN = model_4\n    \nif perf_NN == mean_squared_error(y_hat_5, y_cv):\n    y_hat_NN = np.round(y_hat_5)\n    model_NN = model_5\n    \nif perf_NN == mean_squared_error(y_hat_6, y_cv):\n    y_hat_NN = np.round(y_hat_6)\n    model_NN = model_6\n    \nif perf_NN == mean_squared_error(y_hat_7, y_cv):\n    y_hat_NN = np.round(y_hat_7)\n    model_NN = model_7\n    \nif perf_NN == mean_squared_error(y_hat_8, y_cv):\n    y_hat_NN = np.round(y_hat_8)\n    model_NN = model_8\n    \nif perf_NN == mean_squared_error(y_hat_9, y_cv):\n    y_hat_NN = np.round(y_hat_9)\n    model_NN = model_9","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Support Vector Machine"},{"metadata":{"trusted":true},"cell_type":"code","source":"#build model one\nmodel_SVM_1 = SVC(kernel='rbf')\nmodel_SVM_1.fit(x_train, y_train)\ny_hat_SVM_1 = model_SVM_1.predict(x_cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Mean squared error: ', mean_squared_error(y_hat_SVM_1, y_cv))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#build model two\nmodel_SVM_2 = SVC(kernel='linear')\nmodel_SVM_2.fit(x_train, y_train)\ny_hat_SVM_2 = model_SVM_2.predict(x_cv)\nprint('Mean squared error: ', mean_squared_error(y_hat_SVM_2, y_cv))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#build model three\nmodel_SVM_3 = SVC(kernel='poly')\nmodel_SVM_3.fit(x_train, y_train)\ny_hat_SVM_3 = model_SVM_3.predict(x_cv)\nprint('Mean squared error: ', mean_squared_error(y_hat_SVM_3, y_cv))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#build model four\nmodel_SVM_4 = SVC(kernel='sigmoid')\nmodel_SVM_4.fit(x_train, y_train)\ny_hat_SVM_4 = model_SVM_4.predict(x_cv)\nprint('Mean squared error: ', mean_squared_error(y_hat_SVM_4, y_cv))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"perf_SVM = min(mean_squared_error(y_hat_SVM_1, y_cv), mean_squared_error(y_hat_SVM_2, y_cv), mean_squared_error(y_hat_SVM_3, y_cv), mean_squared_error(y_hat_SVM_4, y_cv))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#choose the best SVM model\nif perf_SVM == mean_squared_error(y_hat_SVM_1, y_cv):\n    model_SVM = model_SVM_1\nif perf_SVM == mean_squared_error(y_hat_SVM_2, y_cv):\n    model_SVM = model_SVM_2\nif perf_SVM == mean_squared_error(y_hat_SVM_3, y_cv):\n    model_SVM = model_SVM_3\nif perf_SVM == mean_squared_error(y_hat_SVM_4, y_cv):\n    model_SVM = model_SVM_4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#y_hat_SVM = model_SVM.predict(y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# KNN "},{"metadata":{"trusted":true},"cell_type":"code","source":"#try 100 different values for k\nfor k in range(1,100):\n    test_model_KNN = KNeighborsClassifier(n_neighbors=k).fit(x_train,y_train)\n    y_hat_test_KNN = test_model_KNN.predict(x_cv)\n    print(\"k: \",k, \"MSE: \",mean_squared_error(y_hat_test_KNN, y_cv))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#seems like around k=30 the MSE is not really improving any further, so we choose k=30\nk=30\nmodel_KNN = KNeighborsClassifier(n_neighbors=k).fit(x_train,y_train)\ny_hat_KNN = model_KNN.predict(x_cv)\nperf_KNN = mean_squared_error(y_hat_KNN, y_cv)\nprint(\"MSE:\", perf_KNN)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Check performance**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#take a look at the different performances\ndata= {'Index':[1,2,3,4],'Model':['Logistic regression', 'Neural Network', 'Support Vector Machine','K nearest neighbors'], 'MSE':[perf_LG, perf_NN, perf_SVM, perf_KNN]}\nperformance_df=pd.DataFrame(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"performance_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#choose the best model\nbest_model_index = performance_df.loc[performance_df['MSE']==performance_df['MSE'].min(), 'Index']\nprint(int(best_model_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let the best model make a prediction for the test data\nif int(best_model_index) == 1:\n    y_hat=model_LG.predict(x_test)\nif int(best_model_index) == 2:\n    y_hat=model_NN.predict(x_test)\nif int(best_model_index) == 3:\n    y_hat=model_SVM.predict(x_test)\nif int(best_model_index) == 4:\n    y_hat=model_KNN.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#formatting y_hat\ny_hat=pd.DataFrame(data=y_hat, columns=['Survived'])\ny_hat=round(y_hat).astype(int)\ny_hat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#merge y_hat and 'PassengerId' together for submission to kaggle.com\nsubmission = pd.DataFrame({'PassengerId': test_df['PassengerId'], 'Survived' : y_hat['Survived']})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check submission length (should be 418)\nlen(submission.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check for invalid values\nsubmission.loc[(submission['Survived']!=0) & (submission['Survived'] != 1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#just in case: make values valid\nsubmission.loc[submission['Survived']<0, 'Survived']=0\nsubmission.loc[submission['Survived']>1, 'Survived']=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#take a final look at the submission\nsubmission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create csv\nsubmission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The submission was uploaded to kaggle.com and scored a 0.78947 accuracy which is good enough for 4685th place out of 220006 who have completed this challenge."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}